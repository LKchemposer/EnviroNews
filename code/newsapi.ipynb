{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewsAPI\n",
    "\n",
    "NewsAPI is a useful tool to get recent (up to 1 month old, 10000 requests/day, for [free](https://newsapi.org/pricing)) news headlines and articles from various news sources and blogs. I used NewsAPI primarily to get URLs from a plethora of sources. [Using NewsAPI on Python](https://newsapi.org/docs/client-libraries/python) is straightforward, but one needs to learn [Lucene syntax](http://www.lucenetutorial.com/lucene-query-syntax.html) to get a comprehensive list of all the related articles.\n",
    "\n",
    "The code here consists of:\n",
    "1. Querying news metadata (most importantly, URLs and sources) from all possible sources\n",
    "2. ... specifically from quality sources: NYT, BBC, Bloomberg, WSJ, Washington Post, Economist, AP, Reuters, Politico, National Geographic, New Scientist, Next Big Future\n",
    "3. ... specifically from conservative sources: Breitbart, Fox News, American Conservative, Washington Times\n",
    "\n",
    "The metadata was stored as pickled dictionaries before dumping them as dataframes. In retrospect, it was definitely not the most efficient method, but it was used because I wanted to be familiar with pickling objects at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import cnfg\n",
    "from IPython import display\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "key = cnfg.load('/Users/lkchemposer/.newsapi_config')\n",
    "newsapi = NewsApiClient(api_key=key['api_key']) # registered from https://newsapi.org/\n",
    "\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucene syntax queries (required by NewsAPI)\n",
    "queries = '''\"environment\" AND\n",
    "            (\"energy\" OR \"green energy\" OR \"go green\" OR\n",
    "             \"pollution\" OR \"air pollution\" OR \"water pollution\" OR \"ocean pollution\" OR \"land pollution\" OR \"noise pollution\" OR\n",
    "             \"waste\" OR \"waste management\" OR\n",
    "             \"water quality\" OR \"air quality\" OR\n",
    "             \"global warming\" OR\n",
    "            (\"global warming\" AND (\"polar bears\" OR \"ice cap melting\")) OR\n",
    "             \"solar energy\" OR \"solar power\" OR \"solar panels\" OR\n",
    "             \"climate change\" OR \"climate march\" OR\n",
    "             \"recycling\" OR\n",
    "             \"endangered species\" OR\n",
    "            (\"electric cars\" AND \"pollution\") OR\n",
    "             \"wind energy\" OR \"geothermal energy\" OR\n",
    "             \"deforestation\" OR\n",
    "            (\"al gore\" AND \"pollution\") OR\n",
    "            (\"planet earth\" OR \"mother earth\" AND \"nature\" AND \"pollution\") OR\n",
    "             \"epa\" OR\n",
    "             \"greenhouse effect\" OR \"greenhouse gases\" OR\n",
    "            (\"fossil fuels\" AND \"pollution\") OR\n",
    "            (\"natural resources\" AND \"pollution\") OR\n",
    "            (\"sutainability\" AND \"green\") OR\n",
    "             \"alternative energy\" OR \"renewable energy\" OR\n",
    "             \"earth day\" OR\n",
    "            (\"carbon dioxide\" AND \"pollution\") OR \"carbon footprint\" OR\n",
    "             \"water conservation\" OR \"energy conservation\" OR \"conservation\" OR\n",
    "             \"electronic waste\" OR \"landfill\" OR \"composting\" OR\n",
    "             \"department of energy\" OR\n",
    "             \"earth science\") OR \n",
    "             \"environmental health\" OR\n",
    "             \"environmental engineer\" OR\n",
    "             \"environmental justice\" OR \"environmental ethics\" OR \"environmental racism\" OR \"environmental sociology\" OR\n",
    "             \"environmental geography\" OR\n",
    "            (\"environmental education\" OR \"environmental studies\" OR \"environmental science\" AND (\"pollution\" OR \"nature\"))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata (with exception handling for null results)\n",
    "def author(article):\n",
    "    try:\n",
    "        return article['author']\n",
    "    except: return\n",
    "\n",
    "def publishedAt(article):\n",
    "    try:\n",
    "        return article['publishedAt']\n",
    "    except: return    \n",
    "\n",
    "def title(article):\n",
    "    try:\n",
    "        return article['title']\n",
    "    except: return   \n",
    "\n",
    "def url(article):\n",
    "    try:\n",
    "        return article['url']\n",
    "    except: return    \n",
    "\n",
    "def source(article):\n",
    "    try:\n",
    "        return article['source']['name']\n",
    "    except: return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... from Any Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsl = []\n",
    "for i in range(1, 100): # maximum 99 pages\n",
    "    req = newsapi.get_everything(q=queries, language='en', sort_by='relevancy', page_size=100, page=i)\n",
    "    articles = req['articles']\n",
    "    for article in articles:\n",
    "        web = url(article)\n",
    "        tle = title(article)\n",
    "        aut = author(article)\n",
    "        pub = publishedAt(article)\n",
    "        sce = source(article)\n",
    "        newsl.append(dict(zip(['url', 'title', 'author', 'publishedAt', 'source'], [web, tle, aut, pub, sce])))\n",
    "    if (i % 10 == 0) | (i == 99): # pickle data\n",
    "        with open('newsapi{}.pkl'.format(i * 100), 'wb') as p:\n",
    "            pickle.dump(newsl, p)  \n",
    "        newsl = [] # reset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.DataFrame(columns=['pub_date', 'source', 'title', 'url'])\n",
    "for i in range(1, 11):\n",
    "    with open('newsapi{}.pkl'.format(i * 1000), 'rb') as p:\n",
    "        articles = pickle.load(p)\n",
    "    for article in articles:\n",
    "        news = news.append({'pub_date': article['publishedAt'],\n",
    "                            'source': article['source'],\n",
    "                            'title': article['title'],\n",
    "                            'url': article['url']}, ignore_index=True)\n",
    "\n",
    "news.drop_duplicates(inplace=True) # cleaning\n",
    "\n",
    "news['pub_date'] = pd.to_datetime(news['pub_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... from Quality Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sces = 'the-new-york-times,bbc-news,bloomberg,the-wall-street-journal,the-washington-post,the-economist,associated-press,reuters,politico,national-geographic,new-scientist,next-big-future'\n",
    "\n",
    "newsl = []\n",
    "for i in range(1, 100):\n",
    "    req = newsapi.get_everything(q=queries, language='en', sort_by='relevancy', page_size=100, page=i, sources=sces)\n",
    "    articles = req['articles']\n",
    "    for article in articles:\n",
    "        web = url(article)\n",
    "        tle = title(article)\n",
    "        aut = author(article)\n",
    "        pub = publishedAt(article)\n",
    "        sce = source(article)\n",
    "        newsl.append(dict(zip(['url', 'title', 'author', 'publishedAt', 'source'], [web, tle, aut, pub, sce])))\n",
    "    if (i % 10 == 0) | (i == 99):\n",
    "        with open('newsapi_legit{}.pkl'.format(i * 100), 'wb') as p:\n",
    "            pickle.dump(newsl, p)  \n",
    "        newsl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit = pd.DataFrame(columns=['pub_date', 'source', 'title', 'url'])\n",
    "for i in range(1, 11):\n",
    "    with open('newsapi_legit{}.pkl'.format(i * 1000), 'rb') as p:\n",
    "        articles = pickle.load(p)\n",
    "    for article in articles:\n",
    "        legit = legit.append({'pub_date': article['publishedAt'],\n",
    "                              'source': article['source'],\n",
    "                              'title': article['title'],\n",
    "                              'url': article['url']}, ignore_index=True)\n",
    "\n",
    "legit.drop_duplicates(inplace=True) # cleaning\n",
    "\n",
    "legit['pub_date'] = pd.to_datetime(legit['pub_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... from Conservative Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sces = 'breitbart-news,fox-news,the-american-conservative,the-washington-times'\n",
    "\n",
    "newsl = []\n",
    "for i in range(1, 100):\n",
    "    req = newsapi.get_everything(q=queries, language='en', sort_by='relevancy', page_size=100, page=i, sources=sces)\n",
    "    articles = req['articles']\n",
    "    for article in articles:\n",
    "        web = url(article)\n",
    "        tle = title(article)\n",
    "        aut = author(article)\n",
    "        pub = publishedAt(article)\n",
    "        sce = source(article)\n",
    "        newsl.append(dict(zip(['url', 'title', 'author', 'publishedAt', 'source'], [web, tle, aut, pub, sce])))\n",
    "    if (i % 10 == 0) | (i == 99):\n",
    "        with open('newsapi_con{}.pkl'.format(i * 100), 'wb') as p:\n",
    "            pickle.dump(newsl, p)  \n",
    "        newsl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pd.DataFrame(columns=['pub_date', 'source', 'title', 'url'])\n",
    "for i in range(1, 11):\n",
    "    with open('newsapi_con{}.pkl'.format(i * 1000), 'rb') as p:\n",
    "        articles = pickle.load(p)\n",
    "    for article in articles:\n",
    "        con = con.append({'pub_date': article['publishedAt'],\n",
    "                          'source': article['source'],\n",
    "                          'title': article['title'],\n",
    "                          'url': article['url']}, ignore_index=True)\n",
    "\n",
    "con.drop_duplicates(inplace=True) # cleaning\n",
    "\n",
    "con['pub_date'] = pd.to_datetime(con['pub_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all dataframes into one\n",
    "\n",
    "env = news.append(legit).append(con)\n",
    "env.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 news sources by volume\n",
    "env['source'].value_counts().head(20).plot('barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
